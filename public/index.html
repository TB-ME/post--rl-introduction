<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <script src="template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
</head>

<body>
  <distill-header></distill-header>
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "Introduction to Reinforcement Learning",
    "description": "Basics of Reinforcement Learning",
    "published": "Jul 13, 2021",
    "authors": [
      {
        "author":"Thomas Bentham",
        "authorURL":"https://tb-me.github.io/"
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <d-title>
    <figure style="grid-column: page; margin: 1rem 0;"><img src="images/heading_image.png"
        style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);" /></figure>
    <p>Reinforcement learning has started being implemented in a variety of fields such as robotics, text mining, trade execution, healthcare,  games and other fields. The novelty of reinforcement learning in comparison to other techniques is that it is highly adaptable as well as it can learn complex complex interactions through exploration.</p>
  </d-title>
  <d-byline></d-byline>
  <d-article>
    <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
    <h2>Introduction</h2>
    <p>Reinforcement Learning at its core attempts to map states to actions which maximize a numerical reward. Unlike supervised learning, which is trained by comparing labelled data to predicted data, and un-supervised learning, which attempts to group unlabelled data together, reinforcement learning learns by maximizing its accumulative long term reward by interacting with the environment.</p>
    <aside>All notation come from <d-cite bibtex-key="suttonReinforcementLearningIntroduction2018"/></aside>

    <a class="marker" href="#section-1.1" id="section-1.1"><span>1.1</span></a>
    <h3>Markov Decision Process (MDP)</h3>
    <p>A Markov Decision Process provides a mathematical framework which can be used to model decision making and consists of the following:
      <ul>
        <li><em>State (<d-math>s</d-math>)</em>: a tensor that can contain a set of vectors and/or scalars which are within the set <d-math>\mathcal{S^+}</d-math><d-footnote><d-math>\mathcal{S^+}</d-math> is a set of all possible states including terminal states, whereas <d-math>\mathcal{S}</d-math> is a set of all possible states that is not including terminal states.</d-footnote>. The state space can either be continuous or discrete. For discrete states, the set <d-math>\mathcal{S^+}</d-math> must contain a finite set of numbers. If the set <d-math>\mathcal{S^+}</d-math> contains a non-finite set of numbers, then the state space is continuous.</li>
        <li><em>Reward (<d-math>R</d-math>)</em>: a scalar that represents a reward for taking the action <d-math>a</d-math> at state <d-math>s</d-math></li>
        <li><em>Action (<d-math>a</d-math>)</em>: a tensor that can contain a set of vectors and/or scalars which are within the set <d-math>\mathcal{A}</d-math>. The action space is continuous when the set <d-math>\mathcal{A}</d-math> contains a non finite set of numbers, else if it contains a finite set of numbers, then the action space if discrete.</li>
        <li><em>State Transition</em> (<d-math>p(s'|s,a)</d-math>): is the probability that the environment transitions from state <d-math>s</d-math> to state <d-math>s'</d-math> when taking the action <d-math>a</d-math>.</li>
      </ul>
    </p>

    <a class="marker" href="#section-1.2" id="section-1.2"><span>1.2</span></a>
    <h3>Core of Reinforcement Learning</h3>
    <p>Reinforcement learning consists of:
      <ul>
        <li><em>Environment</em>: A scenario which accepts any of the possible actions from the set <d-math>\mathcal{A}</d-math> and returns a state (<d-math>s</d-math>) from within the set <d-math>\mathcal{S^+}</d-math> and also a reward.</li>
        <li><em>Policy</em> (<d-math>\pi</d-math>): a policy is a function which performs an action from a set of actions <d-math>\mathcal{A}</d-math> depending on a given state <d-math>s</d-math>.</li>
        <li><em>Agent</em>: consists of a policy (<d-math>\pi</d-math>) and an algorithm which attempts to update the policy to approach the optimal policy <d-math>\pi^*</d-math>.
        <li><em>Episode</em>: A sequence of states and actions that start when time <d-math>t</d-math> is 0 to when it reaches its terminal state at time <d-math>T</d-math>.</li>
        <li><em>Discounted Accumulate Reward (<d-math>G_t</d-math>)</em>: is a performance metric. A discounted performance metric is used to bound the sum to a finite value. Without the discount, for a continuing task with a final time step of <d-math>T = \infty</d-math>, the accumulative reward could be infinite. The agent will not be able to differentiate between a sub-optimal policy with a <d-math>T = \infty</d-math> and the optimal policy <d-math>\pi^*</d-math>. As <d-math>0 < \gamma < 1</d-math>, <d-math>G_t</d-math> will be finite in the equation <d-math>G_t = \sum_{i=0}^{T-t}{\gamma^i R_{t+i}}</d-math>. This means there will be a difference between the sub-optimal policy and <d-math>\pi^*</d-math>.</li>
      </ul>
    </p>
    <p>The agent learns by interacting with the environment to find the optimal policy <d-math>\pi^*</d-math> which performs the action that earns the maximum reward.</p>

    <a class="marker" href="#section-2" id="section-2"><span>2</span></a>
    <h2>Types of Training</h2>
    <ul>
      <li><em>On-Policy</em>: learns from the experiences from the latest policy to update itself to a new policy</li>
      <div style="margin:auto;display:block;text-align: center;"><img src="images/on-policy.png" style="width: 50%;"/><d-cite bibtex-key="levineOfflineReinforcementLearning2020"></d-cite></div>
      <li><em>Off-Policy</em>: typically learns from the experiences of previous policies by storing experiences into a buffer. The buffer is sampled to update the current policy</li>
      <div style="margin:auto;display:block;text-align: center;"><img src="images/off-policy.png" style="width: 50%;"/><d-cite bibtex-key="levineOfflineReinforcementLearning2020"></d-cite></div>
      <li><em>Offline</em>: only uses data collected from other policies and does not further interact from the environment</li>
      <div style="margin:auto;display:block;text-align: center;"><img src="images/offline.png" style="width: 75%;"/><d-cite bibtex-key="levineOfflineReinforcementLearning2020"></d-cite></div>
    </ul>

    <a class="marker" href="#section-2" id="section-3"><span>3</span></a>
    <h2>Value Functions</h2>
    <p>Most reinforcement learning algorithms involve estimating the value functions from states or state-action pairs. A value function is the expected long term reward from the current time step <d-math>t</d-math> with the current state <d-math>s_t</d-math> to the end of the episode. <d-math block> V(s) = \mathbb{E}[G_t | s_t] = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... | s_t]</d-math>
      <ul>
        <li><em>State Value Function</em> (<d-math>V(s)</d-math>): estimates the accumulate discounted reward-to-go from the current state. Thus, the value function is also estimating what actions the policy will take and their rewards. A problem with <d-math>V(s)</d-math>, is that when the policy <d-math>\pi_k</d-math> is updated to <d-math>\pi_{k+1}</d-math>, the value function is still estimating for <d-math>\pi_k</d-math>. If there are non-negligible differences between <d-math>\pi_k</d-math> and <d-math>\pi_{k+1}</d-math>, <d-math>V(s)</d-math> will produce inaccurate value predictions. To avoid this issue, the value function is usually updated more frequently than the policy.</li>
        <li><em>Q-Value or State-Action Value Function</em> (<d-math>Q(s, a)</d-math>): also estimates the discounted reward-to-go but with the state and action. When the action is used for estimation, the dimensionality is increased, meaning a decrease in sample efficiency.</li>
    </ul>
  </p>
  <p> The relationship between <d-math>V(s)</d-math> and <d-math>Q(s,a)</d-math> can be written as: <d-math block>V(s) = \sum_{a \in \mathcal{A}}{\pi(a|s) \cdot Q(s,a)} </d-math>
  <p>The state value function is the probability of taking action <d-math>a</d-math> at state <d-math>s</d-math> multiplied by the Q-value for all possible actions.</p>
  <d-math block>Q(s,a) = \sum_{s' \in \mathcal{S}}{p(s'|s,a) [R(s,a,s') + \gamma V(s')]}</d-math>
  <p>The Q-value is the summation for possible states for <d-math>s'</d-math> of the probability of transitioning from state <d-math>s</d-math> to the next state <d-math>s'</d-math> when taking the action <d-math>a</d-math> multiplied by the reward plus the discounted state value function of state <d-math>s'</d-math></p>





    <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
    <h2>A Brief Survey of Techniques</h2>
    <p>Before diving in: if you haven’t encountered t-SNE before, here’s what you need to know about the math behind it.
      The goal is to take a set of points in a high-dimensional space and find a faithful representation of those points
      in a lower-dimensional space, typically the 2D plane. The algorithm is non-linear and adapts to the underlying
      data, performing different transformations on different regions. Those differences can be a major source of
      confusion.</p>
    <p>This is the first paragraph of the article. Test a long&thinsp;&mdash;&thinsp;dash -- here it is.</p>
    <p>Test for owner's possessive. Test for "quoting a passage." And another sentence. Or two. Some flopping fins; for
      diving.</p>
    <aside>Some text in an aside, margin notes, etc...</aside>
    <p>Here's a test of an inline equation <d-math>c = a^2 + b^2</d-math>. Also with configurable katex standards just
      using inline '$' signs: $$x^2$$ And then there's a block equation:</p>
    <d-math block>
      c = \pm \sqrt{ \sum_{i=0}^{n}{a^{222} + b^2}}
    </d-math>
    <p>Math can also be quite involved:</p>
    <d-math block>
      \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} = 1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}}
      {1+\frac{e^{-6\pi}} {1+\frac{e^{-8\pi}} {1+\cdots} } } }
    </d-math>
    <a class="marker" href="#section-1.1" id="section-1.1"><span>1.1</span></a>
    <h3>Citations</h3>
    <p>
      <d-slider style="width: 200px;"></d-slider>
    </p>
    <p>We can<d-cite bibtex-key="mercier2011humans"></d-cite> also cite <d-cite
        key="gregor2015draw,mercier2011humans,openai2018charter"></d-cite> external publications. <d-cite
        key="dong2014image,dumoulin2016guide,mordvintsev2015inceptionism"></d-cite>. We should also be testing footnotes
      <d-footnote>This will become a hoverable footnote. This will become a hoverable footnote. This will become a
        hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote. This will
        become a hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote.
      </d-footnote>. There are multiple footnotes, and they appear in the appendix<d-footnote>Given I have coded them
        right. Also, here's math in a footnote: <d-math>c = \sum_0^i{x}</d-math>. Also, a citation. Box-ception<d-cite
          key='gregor2015draw'></d-cite>!</d-footnote> as well.</p>
    <a class="marker" href="#section-2" id="section-2"><span>2</span></a>
    <h2>Displaying code snippets</h2>
    <p>Some inline javascript:<d-code language="javascript">var x = 25;</d-code>. And here's a javascript code block.
    </p>
    <d-code block language="javascript">
      var x = 25;
      function(x){
      return x * x;
      }
    </d-code>
    <p>We also support python.</p>
    <d-code block language="python">
      # Python 3: Fibonacci series up to n
      def fib(n):
      a, b = 0, 1
      while a < n: print(a, end=' ' ) a, b=b, a+b </d-code>
        <p>And a table</p>
        <table>
          <thead>
            <tr>
              <th>First</th>
              <th>Second</th>
              <th>Third</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>23</td>
              <td>654</td>
              <td>23</td>
            </tr>
            <tr>
              <td>14</td>
              <td>54</td>
              <td>34</td>
            </tr>
            <tr>
              <td>234</td>
              <td>54</td>
              <td>23</td>
            </tr>
          </tbody>
        </table>
        <d-figure id="last-figure"></d-figure>
        <script>
          const figure = document.querySelector("d-figure#last-figure");
          const initTag = document.createElement("span");
          initTag.textContent = "initialized!"
          figure.appendChild(initTag);
          figure.addEventListener("ready", function () {
            const initTag = figure.querySelector("span");
            initTag.textContent = "ready"
            console.log('ready')
          });
          figure.addEventListener("onscreen", function () {
            const initTag = figure.querySelector("span");
            initTag.textContent = "onscreen"
            console.log('onscreen')
          });
          figure.addEventListener("offscreen", function () {
            const initTag = figure.querySelector("span");
            initTag.textContent = "offscreen!"
            console.log('offscreen')
          });
        </script>
        <p>That's it for the example article!</p>

  </d-article>

  <d-appendix>

    <!-- <h3>Contributions</h3>
    <p>Some text describing who did what.</p>
    <h3>Reviewers</h3>
    <p>Some text with links describing who reviewed the article.</p> -->

    <d-bibliography src="bibliography.bib"></d-bibliography>
  </d-appendix>

  <distill-footer></distill-footer>

</body>