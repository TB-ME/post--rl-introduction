<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <script src="template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
</head>

<body>
  <distill-header></distill-header>
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "Introduction to Reinforcement Learning",
    "description": "Basics of Reinforcement Learning",
    "published": "Jul 13, 2021",
    "authors": [
      {
        "author":"Thomas Bentham",
        "authorURL":"https://tb-me.github.io/"
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <d-title>
    <figure style="grid-column: page; margin: 1rem 0;"><img src="heading_image.png"
        style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);" /></figure>
    <p>TODO: WRITE SOMETHING HERE</p>
  </d-title>
  <d-byline></d-byline>
  <d-article>
    <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
    <h2>Introduction</h2>
    <p>Reinforcement Learning at its core attempts to map states to actions which maximize a numerical reward. Unlike supervised learning, which is trained by comparing labelled data to predicted data, and un-supervised learning, which attempts to group unlabelled data together, reinforcement learning learns by maximizing its accumulative long term reward by interacting with the environment.</p>

    <a class="marker" href="#section-1.1" id="section-1.1"><span>1.1</span></a>
    <h3>Markov Decision Process (MDP)</h3>
    <p>A Markov Decision Process provides a mathematical framework which can be used to model decision making and consists of the following:
      <ul>
        <li><em>State (<d-math>s</d-math>)</em>: a tensor that can contain a set of vectors and/or scalars which are within the set <d-math>\mathcal{S^+}</d-math>. The state space can either be continuous or discrete. For discrete states, the set <d-math>\mathcal{S^+}</d-math> must contain a finite set of numbers. If the set <d-math>\mathcal{S^+}</d-math> contains a non-finite set of numbers, then the state space is continuous.</li>
        <li><em>Reward (<d-math>r</d-math>)</em>: a scalar that represents a reward for taking the action <d-math>a</d-math> at state <d-math>s</d-math></li>
        <li><em>Action (<d-math>a</d-math>)</em>: a tensor that can contain a set of vectors and/or scalars which are within the set <d-math>\mathcal{A}</d-math>. The action space is continuous when the set <d-math>\mathcal{A}</d-math> contains a non finite set of numbers, else if it contains a finite set of numbers, then the action space if discrete.</li>
      </ul>
    </p>

    <a class="marker" href="#section-1.2" id="section-1.2"><span>1.2</span></a>
    <h3>TODO: TITLE</h3>
    <p>Reinforcement learning consists of:
      <ul>
        <li><em>Environment</em>: A scenario which accepts any of the possible actions from the set <d-math>\mathcal{A}</d-math> and returns a state (<d-math>s</d-math>) from within the set <d-math>\mathcal{S^+}</d-math> and also a reward.</li>
        <li><em>Agent</em>: an algorithm which performs an action from a set of action <d-math>\mathcal{A}</d-math> in response from a state.</li>
        <li><em>Episode</em>: A sequence of states and actions that start when time <d-math>t</d-math> is 0 to when it reaches its terminal state at time <d-math>T</d-math>.</li>
        <li><em>Policy</em> (<d-math>\pi</d-math>): </li>
        <li><em>Discounted Accumulate Reward</em>: is a performance metric. A discounted performance metric is used to bound the sum to a finite value. Without the discount, for a continuing task with a final time step of <d-math>T = \infty</d-math>, the accumulative reward could be infinite. The agent will not be able to differentiate between a sub-optimal <d-math>G_t = \sum_{i=0}^{T-t}{\gamma^i r_{t+i}}</d-math></li>
      </ul>
      The agent learns by interacting with the environment to discover which actions earn the maximum accumulate reward.
    </p>

    <a class="marker" href="#section-2" id="section-2"><span>2</span></a>
    <h2></h2>





    <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
    <h2>A Brief Survey of Techniques</h2>
    <p>Before diving in: if you haven’t encountered t-SNE before, here’s what you need to know about the math behind it.
      The goal is to take a set of points in a high-dimensional space and find a faithful representation of those points
      in a lower-dimensional space, typically the 2D plane. The algorithm is non-linear and adapts to the underlying
      data, performing different transformations on different regions. Those differences can be a major source of
      confusion.</p>
    <p>This is the first paragraph of the article. Test a long&thinsp;&mdash;&thinsp;dash -- here it is.</p>
    <p>Test for owner's possessive. Test for "quoting a passage." And another sentence. Or two. Some flopping fins; for
      diving.</p>
    <aside>Some text in an aside, margin notes, etc...</aside>
    <p>Here's a test of an inline equation <d-math>c = a^2 + b^2</d-math>. Also with configurable katex standards just
      using inline '$' signs: $$x^2$$ And then there's a block equation:</p>
    <d-math block>
      c = \pm \sqrt{ \sum_{i=0}^{n}{a^{222} + b^2}}
    </d-math>
    <p>Math can also be quite involved:</p>
    <d-math block>
      \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} = 1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}}
      {1+\frac{e^{-6\pi}} {1+\frac{e^{-8\pi}} {1+\cdots} } } }
    </d-math>
    <a class="marker" href="#section-1.1" id="section-1.1"><span>1.1</span></a>
    <h3>Citations</h3>
    <p>
      <d-slider style="width: 200px;"></d-slider>
    </p>
    <p>We can<d-cite bibtex-key="mercier2011humans"></d-cite> also cite <d-cite
        key="gregor2015draw,mercier2011humans,openai2018charter"></d-cite> external publications. <d-cite
        key="dong2014image,dumoulin2016guide,mordvintsev2015inceptionism"></d-cite>. We should also be testing footnotes
      <d-footnote>This will become a hoverable footnote. This will become a hoverable footnote. This will become a
        hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote. This will
        become a hoverable footnote. This will become a hoverable footnote. This will become a hoverable footnote.
      </d-footnote>. There are multiple footnotes, and they appear in the appendix<d-footnote>Given I have coded them
        right. Also, here's math in a footnote: <d-math>c = \sum_0^i{x}</d-math>. Also, a citation. Box-ception<d-cite
          key='gregor2015draw'></d-cite>!</d-footnote> as well.</p>
    <a class="marker" href="#section-2" id="section-2"><span>2</span></a>
    <h2>Displaying code snippets</h2>
    <p>Some inline javascript:<d-code language="javascript">var x = 25;</d-code>. And here's a javascript code block.
    </p>
    <d-code block language="javascript">
      var x = 25;
      function(x){
      return x * x;
      }
    </d-code>
    <p>We also support python.</p>
    <d-code block language="python">
      # Python 3: Fibonacci series up to n
      def fib(n):
      a, b = 0, 1
      while a < n: print(a, end=' ' ) a, b=b, a+b </d-code>
        <p>And a table</p>
        <table>
          <thead>
            <tr>
              <th>First</th>
              <th>Second</th>
              <th>Third</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>23</td>
              <td>654</td>
              <td>23</td>
            </tr>
            <tr>
              <td>14</td>
              <td>54</td>
              <td>34</td>
            </tr>
            <tr>
              <td>234</td>
              <td>54</td>
              <td>23</td>
            </tr>
          </tbody>
        </table>
        <d-figure id="last-figure"></d-figure>
        <script>
          const figure = document.querySelector("d-figure#last-figure");
          const initTag = document.createElement("span");
          initTag.textContent = "initialized!"
          figure.appendChild(initTag);
          figure.addEventListener("ready", function () {
            const initTag = figure.querySelector("span");
            initTag.textContent = "ready"
            console.log('ready')
          });
          figure.addEventListener("onscreen", function () {
            const initTag = figure.querySelector("span");
            initTag.textContent = "onscreen"
            console.log('onscreen')
          });
          figure.addEventListener("offscreen", function () {
            const initTag = figure.querySelector("span");
            initTag.textContent = "offscreen!"
            console.log('offscreen')
          });
        </script>
        <p>That's it for the example article!</p>

  </d-article>

  <d-appendix>

    <!-- <h3>Contributions</h3>
    <p>Some text describing who did what.</p>
    <h3>Reviewers</h3>
    <p>Some text with links describing who reviewed the article.</p> -->

    <d-bibliography src="bibliography.bib"></d-bibliography>
  </d-appendix>

  <distill-footer></distill-footer>

</body>